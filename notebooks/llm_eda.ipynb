{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaero/.miniforge3/envs/venv-carter/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_models.llama3.api.datatypes import (\n",
    "    CompletionMessage,\n",
    "    StopReason,\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    ")\n",
    "\n",
    "from llama_models.llama3.reference_impl.generation import Llama\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "import torch.distributed.run as torchrun\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"/home/jaero/.llama/checkpoints/Llama3.2-3B-Instruct\"\n",
    "\n",
    "os.path.exists(CHECKPOINT_PATH) and os.path.isdir(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    ckpt_dir: str,\n",
    "    max_seq_len: int = 512,\n",
    "    max_batch_size: int = 4,\n",
    "    model_parallel_size: Optional[int] = None,\n",
    ") -> Llama:\n",
    "    \"\"\"\n",
    "    Examples to run with the models finetuned for chat. Prompts correspond of chat\n",
    "    turns between the user and assistant with the final one always being the user.\n",
    "\n",
    "    An optional system prompt at the beginning to control how the model should respond\n",
    "    is also supported.\n",
    "\n",
    "    `max_gen_len` is optional because finetuned models are able to stop generations naturally.\n",
    "    \"\"\"\n",
    "    generator = Llama.build(\n",
    "        ckpt_dir=ckpt_dir,\n",
    "        max_seq_len=max_seq_len,\n",
    "        max_batch_size=max_batch_size,\n",
    "        model_parallel_size=model_parallel_size,\n",
    "    )\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaero/.miniforge3/envs/venv-carter/lib/python3.11/site-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 4.33 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set required torch.distributed env variables\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\"\n",
    "os.environ['MASTER_ADDR'] = \"127.0.0.1\"\n",
    "os.environ['MASTER_PORT'] = \"29500\"\n",
    "\n",
    "# get llm text generator -- call once per notebook run\n",
    "generator = get_model(CHECKPOINT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text2text_generation.SummarizationPipeline object at 0x7f5c3ec6e990>\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\", device=\"cuda:0\")\n",
    "print(summarizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation(\n",
    "    conversation_history: List[str],\n",
    "    min_length: int = 50,\n",
    "    max_length: int = 300\n",
    ") -> str:\n",
    "    conversation_text = \"\\n\".join(conversation_history)\n",
    "    conv_word_count = len(conversation_text.split(\"\\n\"))\n",
    "    if conv_word_count < max_length:\n",
    "        max_length = 150\n",
    "    summary = summarizer(conversation_text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "\n",
    "def simple_question_answer(\n",
    "    generator: Llama,\n",
    "    dialog: List[Union[UserMessage, CompletionMessage, SystemMessage, StopReason]],\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    "    max_gen_len: Optional[int] = None,\n",
    ") -> any:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        generator (Llama): _description_\n",
    "        dialog (List[Union[UserMessage, CompletionMessage, SystemMessage, StopReason]]): _description_\n",
    "        temperature (float, optional): _description_. Defaults to 0.6.\n",
    "        top_p (float, optional): _description_. Defaults to 0.9.\n",
    "        max_gen_len (Optional[int], optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        any: _description_\n",
    "    \"\"\"\n",
    "    result = generator.chat_completion(\n",
    "        dialog,\n",
    "        max_gen_len=max_gen_len,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    for msg in dialog:\n",
    "        if isinstance(msg, SystemMessage): continue\n",
    "        print(f\"\\n{msg.role.capitalize()}: {msg.content}\\n\")\n",
    "\n",
    "    out_message = result.generation\n",
    "    print(f\"> {out_message.role.capitalize()}: {out_message.content}\")\n",
    "    print(\"\\n==================================\\n\")\n",
    "    \n",
    "    return out_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial prompt\n",
    "context = \"\"\"Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
    "You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "You can return the answer in any order.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: nums = [2,7,11,15], target = 9\n",
    "Output: [0,1]\n",
    "Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].\n",
    "\n",
    "Example 2:\n",
    "\n",
    "Input: nums = [3,2,4], target = 6\n",
    "Output: [1,2]\n",
    "\n",
    "Example 3:\n",
    "\n",
    "Input: nums = [3,3], target = 6\n",
    "Output: [0,1]\"\"\"\n",
    "\n",
    "\n",
    "system_guidance = \"\"\"Follow these rules when giving responses.\n",
    "Imagine you're a technical interviewer and can only give short responses.\n",
    "No matter what, don't provide any answers or code to the user until they've given up. \n",
    "Don't ask the user to consider any improvements or optimizations of their approach until after they have implemented their solution in code.\n",
    "If the user is unsure about your feedback, try and explain the previous response more simply.\n",
    "If the user is asking for hints, first look at evaluating their approach and see if it generally solves the problem (even if it's not an optimal solution).\n",
    "If the user is able to implement a solution that compiles and solves the problem, then ask how they might improve their solution based on Time and Space complexity.\n",
    "Keep this back and forth of question and answering with hints going until the user has either achieved at least a semi-optimal solution or gives up and asks for the answer.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You may assume that each input would have exactly one solution, and you may not use the same element twice. Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. You can return the answer in any order.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_prompt = summarize_conversation([context])\n",
    "summarized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 61. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem: Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
      "You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
      "You can return the answer in any order.\n",
      "\n",
      "Example 1:\n",
      "\n",
      "Input: nums = [2,7,11,15], target = 9\n",
      "Output: [0,1]\n",
      "Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].\n",
      "\n",
      "Example 2:\n",
      "\n",
      "Input: nums = [3,2,4], target = 6\n",
      "Output: [1,2]\n",
      "\n",
      "Example 3:\n",
      "\n",
      "Input: nums = [3,3], target = 6\n",
      "Output: [0,1]\n",
      "\n",
      "User: Hmm so I think I could iterate over each element in the list nums and do a nested for loop to find\n",
      "every pair of numbers and see if they add up to the target amount. How does that sound?\n",
      "\n",
      "\n",
      "> Assistant: Your approach sounds good. You can use two nested loops to iterate over each element in the list and check if the difference between the target and the current element is in the list. \n",
      "\n",
      "However, be aware that this approach has a time complexity of O(n^2) due to the nested loops, which may not be efficient for large lists. \n",
      "\n",
      "Does that make sense?\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "User: I'm not sure ...\n",
      "\n",
      "> Assistant: A time complexity of O(n^2) is not ideal. Can you think of a way to reduce the number of iterations?\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "User: would it be ok if I first tried to code the first approach?\n",
      "\n",
      "> Assistant: Go ahead and try coding the first approach. What's your implementation so far?\n",
      "\n",
      "==================================\n",
      "\n",
      "\n",
      "User: def two_sum(li): \n",
      "\n",
      "> Assistant: To solve this problem, you can use a hash table to store the indices of the numbers in the array as you iterate through it. Here's a high-level overview of the steps:\n",
      "\n",
      "1. Iterate through the array with two pointers, one starting from the beginning and one from the end.\n",
      "2. For each pair of numbers, check if their sum is equal to the target.\n",
      "3. If it is, return the indices of the two numbers.\n",
      "4. If not, move the pointers closer to each other.\n",
      "\n",
      "Can you think of a way to implement this approach in code?\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: def two_sum(li): for n in li: print(n)\n",
      "\n",
      "> Assistant: It looks like you have a basic understanding of the problem. However, your function `two_sum` only prints the elements of the list, but doesn't actually solve the problem.\n",
      "\n",
      "Can you think of a way to modify your function to return the indices of the two numbers that add up to the target?\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nProblem: {context}\")\n",
    "\n",
    "prompt = \"\"\"Hmm so I think I could iterate over each element in the list nums and do a nested for loop to find\n",
    "every pair of numbers and see if they add up to the target amount. How does that sound?\n",
    "\"\"\"\n",
    "\n",
    "context_li = [f\"Problem: {summarized_prompt}\"]\n",
    "\n",
    "dialog = [UserMessage(content=prompt, context=summarize_conversation(context_li)), SystemMessage(content=system_guidance)]\n",
    "out_msg = simple_question_answer(generator, dialog)\n",
    "\n",
    "context_li.append(f\"User: {prompt}\")\n",
    "context_li.append(f\"Agent: {out_msg.content}\")\n",
    "\n",
    "user_query = \"\"\n",
    "\n",
    "while True:\n",
    "    user_query = input()\n",
    "    if \"stop\" in user_query.lower(): break\n",
    "    dialog = [UserMessage(content=user_query, context=summarize_conversation(context_li)), SystemMessage(content=system_guidance)]\n",
    "    out_msg = simple_question_answer(generator, dialog)\n",
    "    \n",
    "    context_li.append(f\"User: {user_query}\")\n",
    "    context_li.append(f\"Agent: {out_msg.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-carter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
