{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaero/.miniforge3/envs/venv-carter/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_models.llama3.api.datatypes import (\n",
    "    CompletionMessage,\n",
    "    StopReason,\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    ")\n",
    "\n",
    "from llama_models.llama3.reference_impl.generation import Llama\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "import torch.distributed.run as torchrun\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"/home/jaero/.llama/checkpoints/Llama3.2-3B-Instruct\"\n",
    "\n",
    "os.path.exists(CHECKPOINT_PATH) and os.path.isdir(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    ckpt_dir: str,\n",
    "    max_seq_len: int = 512,\n",
    "    max_batch_size: int = 4,\n",
    "    model_parallel_size: Optional[int] = None,\n",
    ") -> Llama:\n",
    "    \"\"\"\n",
    "    Examples to run with the models finetuned for chat. Prompts correspond of chat\n",
    "    turns between the user and assistant with the final one always being the user.\n",
    "\n",
    "    An optional system prompt at the beginning to control how the model should respond\n",
    "    is also supported.\n",
    "\n",
    "    `max_gen_len` is optional because finetuned models are able to stop generations naturally.\n",
    "    \"\"\"\n",
    "    generator = Llama.build(\n",
    "        ckpt_dir=ckpt_dir,\n",
    "        max_seq_len=max_seq_len,\n",
    "        max_batch_size=max_batch_size,\n",
    "        model_parallel_size=model_parallel_size,\n",
    "    )\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaero/.miniforge3/envs/venv-carter/lib/python3.11/site-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 4.39 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set required torch.distributed env variables\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\"\n",
    "os.environ['MASTER_ADDR'] = \"127.0.0.1\"\n",
    "os.environ['MASTER_PORT'] = \"29500\"\n",
    "\n",
    "# get llm text generator -- call once per notebook run\n",
    "generator = get_model(CHECKPOINT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text2text_generation.SummarizationPipeline object at 0x7f370807ad90>\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\", device=\"cuda:0\")\n",
    "print(summarizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation(\n",
    "    conversation_history: List[str],\n",
    "    min_length: int = 50,\n",
    "    max_length: int = 300\n",
    ") -> str:\n",
    "    conversation_text = \"\\n\".join(conversation_history)\n",
    "    conv_word_count = len(conversation_text.split(\"\\n\"))\n",
    "    print(f\"Conversation Length: {conv_word_count}\")\n",
    "    if conv_word_count < max_length:\n",
    "        max_length = 150\n",
    "    summary = summarizer(conversation_text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "\n",
    "def simple_question_answer(\n",
    "    generator: Llama,\n",
    "    dialog: List[Union[UserMessage, CompletionMessage, SystemMessage, StopReason]],\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    "    max_gen_len: Optional[int] = None,\n",
    ") -> any:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        generator (Llama): _description_\n",
    "        dialog (List[Union[UserMessage, CompletionMessage, SystemMessage, StopReason]]): _description_\n",
    "        temperature (float, optional): _description_. Defaults to 0.6.\n",
    "        top_p (float, optional): _description_. Defaults to 0.9.\n",
    "        max_gen_len (Optional[int], optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        any: _description_\n",
    "    \"\"\"\n",
    "    result = generator.chat_completion(\n",
    "        dialog,\n",
    "        max_gen_len=max_gen_len,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    for msg in dialog:\n",
    "        print(f\"{msg.role.capitalize()}: {msg.content}\\n\")\n",
    "\n",
    "    out_message = result.generation\n",
    "    print(f\"> {out_message.role.capitalize()}: {out_message.content}\")\n",
    "    print(\"\\n==================================\\n\")\n",
    "    \n",
    "    return out_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial prompt\n",
    "context = \"\"\"Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
    "You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "You can return the answer in any order.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: nums = [2,7,11,15], target = 9\n",
    "Output: [0,1]\n",
    "Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].\n",
    "\n",
    "Example 2:\n",
    "\n",
    "Input: nums = [3,2,4], target = 6\n",
    "Output: [1,2]\n",
    "\n",
    "Example 3:\n",
    "\n",
    "Input: nums = [3,3], target = 6\n",
    "Output: [0,1]\"\"\"\n",
    "\n",
    "\n",
    "system_guidance = \"\"\"Follow these rules when giving responses.\n",
    "Imagine you're a technical interviewer and can only give short responses.\n",
    "No matter what, don't provide any answers or code to the user until they've given up. \n",
    "Don't ask the user to consider any improvements or optimizations of their approach until after they have implemented their solution in code.\n",
    "If the user is asking for hints, first look at evaluating their approach and see if it generally solves the problem (even if it's not an optimal solution).\n",
    "If the user is able to implement a solution that compiles and solves the problem, then ask how they might improve their solution based on Time and Space complexity.\n",
    "Keep this back and forth of question and answering with hints going until the user has either achieved at least a semi-optimal solution or gives up and asks for the answer.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Length: 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You may assume that each input would have exactly one solution, and you may not use the same element twice. Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. You can return the answer in any order.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_prompt = summarize_conversation([context])\n",
    "summarized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 61. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Length: 1\n",
      "User: Hmm so I think I could iterate over each element in the list nums and do a nested for loop to find\n",
      "every pair of numbers and see if they add up to the target amount. How does that sound?\n",
      "\n",
      "\n",
      "System: Follow these rules when giving responses.\n",
      "Imagine you're a technical interviewer and can only give short responses.\n",
      "No matter what, don't provide any answers or code to the user until they've given up. \n",
      "Don't ask the user to consider any improvements or optimizations of their approach until after they have implemented their solution in code.\n",
      "If the user is asking for hints, first look at evaluating their approach and see if it generally solves the problem (even if it's not an optimal solution).\n",
      "If the user is able to implement a solution that compiles and solves the problem, then ask how they might improve their solution based on Time and Space complexity.\n",
      "Keep this back and forth of question and answering with hints going until the user has either achieved at least a semi-optimal solution or gives up and asks for the answer.\n",
      "\n",
      "\n",
      "> Assistant: Your approach sounds reasonable. You can use two nested loops to iterate over each pair of elements in the list `nums`. However, this approach has a time complexity of O(n^2), which might not be efficient for large inputs.\n",
      "\n",
      "Before you proceed with implementing your solution, let's consider an alternative approach. What if you could use a hash table to store the elements of `nums` as you iterate over them, and then check if the complement of each element (i.e., the value that needs to be added to it to reach the target) is already in the hash table? This approach would reduce the time complexity to O(n). Can you think of how you might implement this approach?\n",
      "\n",
      "==================================\n",
      "\n",
      "Conversation Length: 7\n",
      "User: That sounds interesting. I guess I could do an initial loop to store all of the unique numbers in a hash table?\n",
      "\n",
      "System: Follow these rules when giving responses.\n",
      "Imagine you're a technical interviewer and can only give short responses.\n",
      "No matter what, don't provide any answers or code to the user until they've given up. \n",
      "Don't ask the user to consider any improvements or optimizations of their approach until after they have implemented their solution in code.\n",
      "If the user is asking for hints, first look at evaluating their approach and see if it generally solves the problem (even if it's not an optimal solution).\n",
      "If the user is able to implement a solution that compiles and solves the problem, then ask how they might improve their solution based on Time and Space complexity.\n",
      "Keep this back and forth of question and answering with hints going until the user has either achieved at least a semi-optimal solution or gives up and asks for the answer.\n",
      "\n",
      "\n",
      "> Assistant: Yes, using a hash table to store unique numbers and their indices can help reduce the time complexity. \n",
      "\n",
      "What do you think about using a hash table to store the numbers you've seen so far and their indices?\n",
      "\n",
      "==================================\n",
      "\n",
      "Conversation Length: 11\n",
      "User: That's a good idea too! Wait... why is that good?\n",
      "\n",
      "System: Follow these rules when giving responses.\n",
      "Imagine you're a technical interviewer and can only give short responses.\n",
      "No matter what, don't provide any answers or code to the user until they've given up. \n",
      "Don't ask the user to consider any improvements or optimizations of their approach until after they have implemented their solution in code.\n",
      "If the user is asking for hints, first look at evaluating their approach and see if it generally solves the problem (even if it's not an optimal solution).\n",
      "If the user is able to implement a solution that compiles and solves the problem, then ask how they might improve their solution based on Time and Space complexity.\n",
      "Keep this back and forth of question and answering with hints going until the user has either achieved at least a semi-optimal solution or gives up and asks for the answer.\n",
      "\n",
      "\n",
      "> Assistant: Let's start fresh.\n",
      "\n",
      "You're given an array of integers `nums` and a target integer `target`. You need to find two numbers in the array that add up to the target. The catch is that you can't use the same number twice.\n",
      "\n",
      "What approach are you considering?\n",
      "\n",
      "==================================\n",
      "\n",
      "Conversation Length: 17\n",
      "User: Ohh I see so by including the indices I can ensure that I've used a specific number at a specific index exactly once.\n",
      "\n",
      "System: Follow these rules when giving responses.\n",
      "Imagine you're a technical interviewer and can only give short responses.\n",
      "No matter what, don't provide any answers or code to the user until they've given up. \n",
      "Don't ask the user to consider any improvements or optimizations of their approach until after they have implemented their solution in code.\n",
      "If the user is asking for hints, first look at evaluating their approach and see if it generally solves the problem (even if it's not an optimal solution).\n",
      "If the user is able to implement a solution that compiles and solves the problem, then ask how they might improve their solution based on Time and Space complexity.\n",
      "Keep this back and forth of question and answering with hints going until the user has either achieved at least a semi-optimal solution or gives up and asks for the answer.\n",
      "\n",
      "\n",
      "> Assistant: You're on the right track by considering the indices. \n",
      "\n",
      "One approach is to use a hash table to store the numbers we've seen so far and their indices. \n",
      "\n",
      "When we find a pair that adds up to the target, return their indices. \n",
      "\n",
      "But how do we ensure we don't use the same element twice? \n",
      "\n",
      "Can you think of a way to modify the hash table to prevent this?\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Hmm so I think I could iterate over each element in the list nums and do a nested for loop to find\n",
    "every pair of numbers and see if they add up to the target amount. How does that sound?\n",
    "\"\"\"\n",
    "\n",
    "context_li = [f\"Problem: {summarized_prompt}\"]\n",
    "\n",
    "dialog = [UserMessage(content=prompt, context=summarize_conversation(context_li)), SystemMessage(content=system_guidance)]\n",
    "out_msg = simple_question_answer(generator, dialog)\n",
    "\n",
    "context_li.append(f\"User: {prompt}\")\n",
    "context_li.append(f\"Agent: {out_msg.content}\")\n",
    "\n",
    "user_query = \"\"\n",
    "\n",
    "while True:\n",
    "    user_query = input()\n",
    "    if \"stop\" in user_query.lower(): break\n",
    "    dialog = [UserMessage(content=user_query, context=summarize_conversation(context_li)), SystemMessage(content=system_guidance)]\n",
    "    out_msg = simple_question_answer(generator, dialog)\n",
    "    \n",
    "    context_li.append(f\"User: {user_query}\")\n",
    "    context_li.append(f\"Agent: {out_msg.content}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-carter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
